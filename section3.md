# DRBDおよびcLVM

## DRBDとは？

Distributed Replicated Block Device

ブロックデバイスのミラーリング機能を提供

ネットワークを使用したRAID1みたいな感じ

カーネルモジュールとして提供されている

ファイルシステムの制約はないがジャーナルファイルシステム推奨

カーネルモジュールはdrbd

低レベルデバイスとして物理ハードディスク、LVM、EMVSボリュームが使用可能

### DRBDのリソースとロール

データの属性をリソースと呼ぶ

構成要素は

* リソース名
* ボリューム
* DRBDデバイス
* コネクション

リソースはプライマリ、セカンダリのいずれかのロールを持つ

以下のモードで実行する

* 単一プライマリモード
* デュアルプライマリモード

デフォルトでは1台のノードのみがプライマリロールをもつ

プライマリロールのDRBDデバイスでは読み書きの制約なし

セカンダリロールのDRBDデバイスでは対応するプライマリノードからのデータ更新のみを受け付ける。自ノードのアプリケーションからのデータの読み書きは一切できない

切り替えは手動、クラスタ管理アプリケーションによる自動切り替えに対応

デュアルプライマリモードではすべてのリソースが任意の時点でプライマリロールを持つ

2つのノードから同一データに対して同時アクセスが可能。

そのためGFS2やOCFS2のような分散ロックマネージャを持つ分散クラスタファイルシステムが必須

### リソースのステータス情報

```
/proc/drbd
```

という仮想ファイルシステムに書き込まれる

### PacemakerクラスタでのDRBDの使用

以下のリソースを設定、管理する必要あり

* レプリケーションリソース（クラスタサービス）
* クラスタサービスにたいする仮想IPアドレス
* DRBDストレージをマウントするためのファイルシステムリソース

### レプリケーションモード

通信において同期、非同期に対応していて以下のレプリケーションモードで動作可能

* プロトコルA　非同期　障害耐性低い、レイテンシ高い
* プロトコルB　メモリ同期（半非同期）耐障害性中、レイテンシ中
* プロトコルC　動機レプリケーション　耐障害性高い、レイテンシ長い

AではTCP送信バッファに送られた時点で書き込み完了とする
Bではセカンダリノードに届いた時点で書き込み完了としている
Cではセカンダリノードのディスクに書き込まれた時点で完了としている

### 3ノードレプリケーション

冗長性を高めるために3番目のノードを追加できる

用途は主にバックアップやディザスタリカバリ

プロトコルCだとネットワーク遅延によりレイテンシが遅くなる可能性があるのでAを利用するなどする

### DRBDのデュアルプライマリモードによる負荷分散

```
/etc/drbd.conf
```

という設定ファイルに

resource > netセクションで

```
allow-two-primaries
```
オプションを設定してデュアルプライマリモードにする

## DRBDの管理用ユーティリティ

* drbdadm debdsetup,debdmetaのフロントエンド
* drbdsetup drbdモジュールの設定
* drbdmeta DRBDメタデータの作成や変更、ダンプ、リストアを行う

### drbdadm

drbdadm オプション サブコマンド 引数


コマンド例

* role 両ノードのデバイスの現在のロールを表示
* cstate 両ノードのデバイスの接続状態を表示
* verify オンライン照合を実行

DRBDのリソースを有効にするには以下のとおり実行

```
drbdadm attach リソース名
drbdadm syncer リソース名
drbdadm connect リソース名
```

## cLVMとは？

Clusterd Logical Volume Manager

LVMのクラスタ用拡張

クラスタ内の各ノードでclvmdデーモンが動作してすべてのノードで利用可能な共有ストレージ上で動作

## cLVMユーティリティ

### vgsコマンド

ボリュームグループ一覧を表示

### vgchangeコマンド

ボリュームグループの属性を変更する

例：ボリュームグループを共有するかは

```
vgchange -c [yn]
```

### クラスタファイルシステム

* 同時アクセスにおける排他制御や同期
* 透過的レプリケーション
* 耐障害性

の機能を提供する

複数のノードから同時アクセス可能

障害時にも可能な限り更新上場の反映やロック解除などの適切な処理を試みる仕組みを備える

* GFS2
* OCFS2

がLinuxにおいて代表的

NFS複数ノードからアクセス可能。スケーラビリティは高いが不整合は許容される仕組みなので

* 整合性が重要なアプリケーションを動かすActive-Activeクラスタ
* フェイルオーバークラスタ
* ファイルのロックを多用するようなアプリケーションは

はクラスタファイルシステムを採用すべき

### GFS2(Global File System2)

Linuxカーネルのファイルシステムインターフェース（VFS層）と連動する

DLM,cLVMと併用することでクラスタ環境での利用を最適化する

マウントするクラスタ内の各ノードはそれぞれジャーナルデータを持つ必要がある

マウント済みのGFS2に対してジャーナルを追加するには

```
gfs2_jadd
```

### GFS2のコマンド

* gfs2_grow マウントされたファイルシステムの拡張
* gfs2_edit 内部構造の表示、編集
* gfs2_jadd ジャーナル追加

縮小はできない

## OCFS2

Oracle Cluster Filesystem ver.2

クラスタソフトウェアと組み合わせて利用可能

### OCFS2のコマンド

* o2info
* o2image

### OCFS2のパラメータ

tunefs.ocfs2で変更可能

### O2CBクラスタスタック

OCFS2のデフォルトのクラスタスタック

* o2nm ノードマネージャ
* o2hb Heartbeatエージェント
* o2net ネットワークエージェント
* o2dlm 分散ロックマネージャ
* dlmfs インメモリファイルシステム

設定ファイルは
 
```
/etc/ocfs2/cluster.conf
/etc/sysconfig/o2cb
```

### /etc/ocf2/cluster.confファイル

OCFS2ファイルシステムを使用したクラスタの配置に関する設定

クラスタ設定項目

* node_count
* name

ノード設定項目

* ip_port
* ip_address
* number
* name
* cluster

### /etc/sysconfig/o2cbファイル

O2CBクラスタスタックに関する基本設定のファイル

```
o2cb configure
```
コマンドで設定可能

例：heartbeatと統合する場合

```
O2CB_HEARTBEAT_MODE="user"
```

### その他の分散ファイルシステム

### GlusterFS

FUSEを利用した実装で導入が容易

### CephFS

POSIX準拠

OpenStackのブロックストレージCinderのバックエンドなどで利用される

オブジェクトのメタデータをメモリ上のキャッシュ専用のメタデータサーバMDSで並列処理することによりパフォーマンスを出せる

### AFS

セキュリティ、キャッシング、スケーラビリティに優れる
